# -*- coding: utf-8 -*-
"""TradingSystemTester

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FmODr4UaJMsHcGErqrHE1f4myx6H24QJ
"""

# ========================
# SECTION 1: SETUP & IMPORTS
# ========================

# For Colab, uncomment this line:
# !pip install requests pandas numpy matplotlib seaborn torch torchvision scikit-learn plotly

# Check CUDA availability
import torch
print(f" CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f" GPU: {torch.cuda.get_device_name(0)}")
    print(f" CUDA Version: {torch.version.cuda}")
    device = torch.device("cuda")
else:
    print(" Using CPU (CUDA not available)")
    device = torch.device("cpu")

import requests
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Deep Learning
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random

# ========================
# SECTION 2: EXPERIENCE REPLAY
# ========================

class Experience:
    def __init__(self, state, preference_vector, action, new_state, reward):
        self.state = state
        self.preference_vector = preference_vector
        self.action = action
        self.new_state = new_state
        self.reward = reward

class ExperienceReplayBuffer:
    """Near-on Experience Replay Buffer from MOSEC paper"""
    def __init__(self, capacity=10000, similarity_threshold=0.1):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.similarity_threshold = similarity_threshold

    def add(self, state, preference_vector, action, new_state, reward):
        """Add experience to buffer"""
        experience = Experience(state, preference_vector, action, new_state, reward)
        self.buffer.append(experience)

    def sample(self):
        """Sample random experience from buffer"""
        if len(self.buffer) == 0:
            return None
        return random.choice(self.buffer)

    def size(self):
        return len(self.buffer)

# ========================
# SECTION 3: NER STRATEGY
# ========================

def compute_similarity(cur_state, sampled_state, cur_pref, sampled_pref):
    state_similarity = np.linalg.norm(cur_state - sampled_state, ord=2)
    preference_similarity = np.linalg.norm(cur_pref - sampled_pref, ord=2)
    return state_similarity, preference_similarity

def Sample_NER(sample_coef, batch_size, exp_replay, cur_pref, cur_state):
    if exp_replay.size() == 0:
        return []

    samples_size = min(int(sample_coef * batch_size), exp_replay.size())
    exp_list = []  # (experience, similarity)

    for _ in range(samples_size):
        sample_exp = exp_replay.sample()
        if sample_exp is None:
            continue
        state_sim_score, pref_sim_score = compute_similarity(cur_state, sample_exp.state, cur_pref, sample_exp.preference_vector)
        total_score = state_sim_score + pref_sim_score
        exp_list.append((sample_exp, total_score))

    ordered_exp_list = sorted(exp_list, key=lambda x: x[1])
    return ordered_exp_list[:samples_size]

def generate_random_preference_vector():
    """
    Generates a random preference vector of length 3,
    representing weights for:
    - Profit
    - energy saving
    - compute usage

    The vector components sum to exactly 1.
    """
    # Generate 3 random positive values
    raw_values = [random.random() for _ in range(3)]

    # Normalize so sum is 1
    total = sum(raw_values)
    normalized_vector = np.array([v / total for v in raw_values])
    return normalized_vector

# ========================
# SECTION 4: EF MEMORY
# ========================

class EFMemory:
    """
    EFMemory
    ---------
    • record(ep, vec)  – save a (episode, preference_vector) pair
    • earliest()       – first vector ever stored  (smallest episode)
    • latest()         – most recently stored vector (largest episode)
    """
    def __init__(self):
        self.history = []  # [(episode, vector), …]

    def record(self, episode, vector):
        self.history.append((episode, vector))
        self.history.sort(key=lambda x: x[0])  # keep oldest first

    def earliest(self):
        return self.history[0][1] if self.history else generate_random_preference_vector()

    def latest(self):
        return self.history[-1][1] if self.history else generate_random_preference_vector()

# ========================
# SECTION 5: MULTI-OBJECTIVE DQN
# ========================

class MultiObjectiveDQN(nn.Module):
    """Multi-Objective Deep Q-Network - FIXED VERSION"""
    def __init__(self, state_dim=6, preference_dim=3, action_dim=9, hidden_dim=128):
        super(MultiObjectiveDQN, self).__init__()

        self.state_dim = state_dim
        self.preference_dim = preference_dim
        self.action_dim = action_dim

        # State processing network
        self.state_net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Preference processing network
        self.preference_net = nn.Sequential(
            nn.Linear(preference_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, hidden_dim//2),
            nn.ReLU()
        )

        # Combined processing network
        self.combined_net = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )

    def forward(self, state, preference_vector):
        """Forward pass through the network"""
        # Convert to tensors if numpy arrays
        if isinstance(state, np.ndarray):
            state = torch.FloatTensor(state).to(device)
            if len(state.shape) == 1:
                state = state.unsqueeze(0)
        if isinstance(preference_vector, np.ndarray):
            preference_vector = torch.FloatTensor(preference_vector).to(device)
            if len(preference_vector.shape) == 1:
                preference_vector = preference_vector.unsqueeze(0)

        # Process state and preference separately
        state_features = self.state_net(state)
        preference_features = self.preference_net(preference_vector)

        # Combine features
        combined_features = torch.cat([state_features, preference_features], dim=-1)

        # Output Q-values
        q_values = self.combined_net(combined_features)

        return q_values

# ========================
# SECTION 6: UPDATED SITE RESOURCES & SAMPLE DATA
# ========================

# Updated inventory with air2 miners
INVENTORY = {
    "inference": {
        "asic": {
            "power": 15000,
            "tokens": 50000
        },
        "gpu": {
            "power": 5000,
            "tokens": 1000
        }
    },
    "miners": {
        "air": {
            "hashrate": 1000,
            "power": 3500
        },
        "air2": {
            "hashrate": 1000,
            "power": 3500
        },
        "hydro": {
            "hashrate": 5000,
            "power": 5000
        },
        "immersion": {
            "hashrate": 10000,
            "power": 10000
        }
    }
}

# Sample state from the research/API
sample_state = {
    "energy_price": 1.38850837489261,
    "hash_price": 3.05898030398031,
    "token_price": 0.655874531232632
}

# Sample site configuration representing /machines endpoint
sample_site = {
    "site_id": 0,
    "id": 988,
    "air_miners": 0,
    "hydro_miners": 0,
    "immersion_miners": 5,
    "gpu_compute": 10,
    "asic_compute": 5,
    "updated_at": "2025-06-21T23:36:07.432517",
    "total_power_used": 133330,
    "total_power_cost": 186145.33397570328,
    "total_revenue": 73678.38520133552,
    "power": {
        "air_miners": 0,
        "hydro_miners": 0,
        "immersion_miners": 50000,
        "gpu_compute": 33330,
        "asic_compute": 50000
    },
    "revenue": {
        "air_miners": 0.0,
        "hydro_miners": 0.0,
        "immersion_miners": 47001.90428169591,
        "gpu_compute": 7621.851691325603,
        "asic_compute": 19054.629228314006
    }
}

# Updated action space: (inference_ratio, mining_ratio) pairs
# These represent percentage allocation of total power
ACTION_SPACE_RATIOS = [
    (0.0, 1.0),   # 0% inference, 100% mining
    (0.1, 0.9),   # 10% inference, 90% mining
    (0.2, 0.8),   # 20% inference, 80% mining
    (0.3, 0.7),   # 30% inference, 70% mining
    (0.4, 0.6),   # 40% inference, 60% mining
    (0.5, 0.5),   # 50% inference, 50% mining
    (0.6, 0.4),   # 60% inference, 40% mining
    (0.7, 0.3),   # 70% inference, 30% mining
    (0.8, 0.2),   # 80% inference, 20% mining
    (0.9, 0.1),   # 90% inference, 10% mining
    (1.0, 0.0),   # 100% inference, 0% mining
]

# ========================
# SECTION 7: ENHANCED ENVIRONMENT - FIXED RESEARCH VERSION
# ========================

class Environment:
    """Enhanced Environment based on research implementation"""

    P_TOTAL = 1_000_000  # total available watts (1MW)

    def __init__(self):
        self.inventory = INVENTORY
        self.site = sample_site.copy()  # Available machines on site
        self.time_step = 0

    def get_simulated_prices(self):
        """Simulate fluctuating market prices"""
        # Simulate price fluctuations based on sample_state
        base_time = time.time() + self.time_step * 300  # 5-minute intervals

        # Start from research sample prices and add fluctuations
        energy_price = sample_state["energy_price"] + 0.1 * np.sin(base_time / 1000) + 0.05 * np.random.randn()
        hash_price = sample_state["hash_price"] + 0.3 * np.cos(base_time / 1500) + 0.1 * np.random.randn()
        token_price = sample_state["token_price"] + 0.1 * np.sin(base_time / 2000) + 0.05 * np.random.randn()

        return {
            "energy_price": max(0.5, energy_price),
            "hash_price": max(2.0, hash_price),
            "token_price": max(0.4, token_price)
        }

    def get_current_state(self):
        """Get current state representation"""
        prices = self.get_simulated_prices()

        # State: [energy_price, hash_price, token_price, power_usage_ratio, mining_efficiency, token_efficiency]
        current_power_usage = self.site.get("total_power_used", 0)
        power_ratio = current_power_usage / self.P_TOTAL

        # Calculate efficiency metrics
        mining_efficiency = self.site.get("revenue", {}).get("immersion_miners", 0) / max(1, self.site.get("power", {}).get("immersion_miners", 1))
        token_efficiency = self.site.get("revenue", {}).get("asic_compute", 0) / max(1, self.site.get("power", {}).get("asic_compute", 1))

        state = np.array([
            prices["energy_price"],
            prices["hash_price"],
            prices["token_price"],
            power_ratio,
            mining_efficiency / 1000,  # Normalize
            token_efficiency / 1000    # Normalize
        ])

        return state

    def step(self, action, pref_vector):
        """Execute action using greedy allocation strategy from research"""
        self.time_step += 1

        # Get action ratios
        if action >= len(ACTION_SPACE_RATIOS):
            action = 0
        infer_ratio, mining_ratio = ACTION_SPACE_RATIOS[action]

        # Get current prices
        prices = self.get_simulated_prices()

        # Calculate power allocations
        P_infer_rem = infer_ratio * self.P_TOTAL
        P_mine_rem = mining_ratio * self.P_TOTAL

        # Check if allocation exceeds total power constraint
        if (P_infer_rem + P_mine_rem) > self.P_TOTAL:
            # Invalid action - penalize heavily
            reward = -10000
            return self.get_current_state(), reward

        # ==========================================
        # GREEDY INFERENCE ALLOCATION (from research)
        # ==========================================

        # Sort inference machines by tokens-per-watt descending
        inf_order = sorted(
            self.inventory['inference'].items(),
            key=lambda kv: kv[1]['tokens'] / kv[1]['power'],
            reverse=True
        )

        power_inf_used = 0
        tokens_generated = 0

        for inf_name, specs in inf_order:
            unit_p = specs['power']
            unit_t = specs['tokens']
            # How many you actually have on-site
            available = self.site.get(f"{inf_name}_compute", 0)
            # How many you *could* power
            max_by_power = int(P_infer_rem // unit_p) if unit_p > 0 else 0
            n = min(available, max_by_power)
            if n <= 0:
                continue
            # Consume power & produce tokens
            used = n * unit_p
            power_inf_used += used
            tokens_generated += n * unit_t
            P_infer_rem -= used
            # Stop if no more inference power budget
            if P_infer_rem < min([m['power'] for m in self.inventory['inference'].values()]):
                break

        # ==========================================
        # GREEDY MINING ALLOCATION (from research)
        # ==========================================

        # Sort miners by (hashrate-per-watt, hashrate) descending
        mine_order = sorted(
            self.inventory['miners'].items(),
            key=lambda kv: (kv[1]['hashrate'] / kv[1]['power'], kv[1]['hashrate']),
            reverse=True
        )

        power_mine_used = 0
        hash_generated = 0

        for mine_name, specs in mine_order:
            unit_p = specs['power']
            unit_h = specs['hashrate']
            available = self.site.get(f"{mine_name}_miners", 0)
            max_by_power = int(P_mine_rem // unit_p) if unit_p > 0 else 0
            n = min(available, max_by_power)
            if n <= 0:
                continue
            used = n * unit_p
            power_mine_used += used
            hash_generated += n * unit_h
            P_mine_rem -= used
            if P_mine_rem < min([m['power'] for m in self.inventory['miners'].values()]):
                break

        # ==========================================
        # PRICING & REVENUE CALCULATION (from research)
        # ==========================================

        ep = prices['energy_price']
        hp = prices['hash_price']
        tp = prices['token_price']

        mining_revenue = hash_generated * hp
        infer_revenue = tokens_generated * tp
        total_revenue = mining_revenue + infer_revenue

        # COSTS
        total_cost = self.P_TOTAL * ep  # Pay for full power capacity

        # METRICS
        profit = total_revenue - total_cost
        energy_usage = power_inf_used + power_mine_used
        compute_idle = self.P_TOTAL - energy_usage

        # ========================================
        # MULTI-OBJECTIVE REWARD (FIXED VERSION)
        # ========================================

        # Fix: Use array indexing instead of dot notation
        reward = (
            profit * pref_vector[0] -              # profit preference
            energy_usage * pref_vector[1] * 0.001 - # energy saving preference
            compute_idle * pref_vector[2] * 0.0001   # compute usage preference
        )

        # Update site state for next iteration
        self.site.update({
            "total_power_used": int(energy_usage),
            "total_power_cost": total_cost,
            "total_revenue": total_revenue,
            "power": {
                "inference_used": int(power_inf_used),
                "mining_used": int(power_mine_used)
            },
            "revenue": {
                "mining_revenue": mining_revenue,
                "inference_revenue": infer_revenue
            }
        })

        print(f" Power: Inference={power_inf_used:.0f}W, Mining={power_mine_used:.0f}W, Total={energy_usage:.0f}W")
        print(f" Revenue: Mining=${mining_revenue:.2f}, Inference=${infer_revenue:.2f}, Cost=${total_cost:.2f}")
        print(f" Reward: {reward:.2f} (Profit={profit:.2f}, Usage={energy_usage:.0f}, Idle={compute_idle:.0f})")

        # Generate next state
        new_state = self.get_current_state()

        return new_state, reward

# ========================
# SECTION 8: AGENT - UPDATED FOR NEW ACTION SPACE
# ========================

class Agent:
    def __init__(self):
        # Updated action dimension for ratio-based actions
        action_dim = len(ACTION_SPACE_RATIOS)

        self.QNet = MultiObjectiveDQN(state_dim=6, preference_dim=3, action_dim=action_dim).to(device)
        self.target_QNet = MultiObjectiveDQN(state_dim=6, preference_dim=3, action_dim=action_dim).to(device)

        # Copy weights to target network
        self.target_QNet.load_state_dict(self.QNet.state_dict())

        # Optimizer
        self.optimizer = optim.Adam(self.QNet.parameters(), lr=0.001)

def epsilon_greedy(Q_values, epsilon):
    """
    Select an action using the ε-greedy strategy.
    """
    if random.random() < epsilon:
        # Explore: random action
        return random.randint(0, Q_values.shape[-1] - 1)
    else:
        # Exploit: best Q-value
        return Q_values.argmax().item()

# Updated action dimension
action_dim = len(ACTION_SPACE_RATIOS)

# ========================
# SECTION 9: LEARNING LOOP - FIXED
# ========================

def Learning_Loop_with_Adaptation(ep_num, site_num, sample_coef=1.0):
    """Enhanced learning loop with adaptation error tracking"""

    # Initialize components
    agent = Agent()
    environment = Environment()
    ef_memory = EFMemory()
    exp_replay = ExperienceReplayBuffer(capacity=10000)
    adaptation_tracker = AdaptationErrorTracker(window_size=10)

    gamma = 0.99
    batch_size = 32
    epsilon = 0.1
    epsilon_decay = 0.995
    epsilon_min = 0.01

    episode_rewards = []
    adaptation_errors = []
    previous_pref = None

    print(" Starting Multi-Objective RL Training with Adaptation Error...")

    for ep in range(ep_num):
        print(f"\n Episode {ep+1}/{ep_num}")

        # Generate preference vector for this episode
        select_pref_vect = generate_random_preference_vector()
        ef_memory.record(ep, select_pref_vect)

        # Calculate adaptation error if preference changed
        adaptation_error = 0.0
        if previous_pref is not None:
            adaptation_error = calculate_adaptation_error(
                agent, environment, select_pref_vect, previous_pref, adaptation_tracker
            )
            adaptation_errors.append(adaptation_error)

        episode_reward = 0

        for site in range(site_num):
            # Get current state
            cur_state = environment.get_current_state()

            # Get Q-values and select action
            Q_vals = agent.QNet(cur_state, select_pref_vect)
            action = epsilon_greedy(Q_vals, epsilon)

            # Execute action (now using ratio-based allocation)
            new_state, base_reward = environment.step(action, select_pref_vect)

            # Apply adaptation error penalty
            adaptation_penalty = adaptation_tracker.get_adaptation_penalty()
            final_reward = base_reward - adaptation_penalty

            episode_reward += final_reward

            # Track adaptation
            adaptation_tracker.add_experience(select_pref_vect, final_reward)

            # Store experience
            exp_replay.add(cur_state, select_pref_vect, action, new_state, final_reward)

            # Training step if we have enough experiences
            if exp_replay.size() >= batch_size:
                # Sample experiences using NER
                trans = Sample_NER(sample_coef, batch_size, exp_replay, select_pref_vect, cur_state)

                if len(trans) > 0:
                    # Get earliest preference for dual learning
                    ef_pref_vect = ef_memory.earliest()

                    losses = []

                    for exp_data, similarity in trans:
                        exp = exp_data

                        # Convert to tensors
                        s_i = torch.FloatTensor(exp.state).unsqueeze(0).to(device)
                        a_i = exp.action
                        r_i = torch.FloatTensor([exp.reward]).to(device)
                        s_next_i = torch.FloatTensor(exp.new_state).unsqueeze(0).to(device)

                        # Q-values for current preferences
                        Q_select_pref = agent.QNet(s_i, select_pref_vect)
                        Q_ef_pref = agent.QNet(s_i, ef_pref_vect)

                        Q_select_pref_i = Q_select_pref[0, a_i]
                        Q_ef_pref_i = Q_ef_pref[0, a_i]

                        # Target Q-values
                        with torch.no_grad():
                            target_Q_select = agent.target_QNet(s_next_i, select_pref_vect)
                            target_Q_ef = agent.target_QNet(s_next_i, ef_pref_vect)

                            target_Q_select_max = target_Q_select.max(dim=-1)[0]
                            target_Q_ef_max = target_Q_ef.max(dim=-1)[0]

                            y_select_pref_i = r_i + gamma * target_Q_select_max
                            y_ef_pref_i = r_i + gamma * target_Q_ef_max

                        # Loss for this transition
                        loss_i = 0.5 * (
                            F.mse_loss(Q_select_pref_i, y_select_pref_i) +
                            F.mse_loss(Q_ef_pref_i, y_ef_pref_i)
                        )
                        losses.append(loss_i)

                    if losses:
                        # Backprop update
                        total_loss = torch.stack(losses).mean()
                        agent.optimizer.zero_grad()
                        total_loss.backward()
                        agent.optimizer.step()

        # Update target network periodically
        if ep % 10 == 0:
            agent.target_QNet.load_state_dict(agent.QNet.state_dict())
            print(f" Target network updated at episode {ep}")

        # Decay epsilon
        epsilon = max(epsilon_min, epsilon * epsilon_decay)

        episode_rewards.append(episode_reward)

        # Update previous preference
        previous_pref = select_pref_vect.copy()

        print(f" Episode Reward: {episode_reward:.2f}")
        print(f" Preference Vector: Profit={select_pref_vect[0]:.2f}, Energy={select_pref_vect[1]:.2f}, Compute={select_pref_vect[2]:.2f}")
        print(f" Adaptation Error: {adaptation_error:.3f}")
        print(f" Epsilon: {epsilon:.3f}")

        # Show progress every 20 episodes
        if (ep + 1) % 20 == 0:
            avg_reward = np.mean(episode_rewards[-20:])
            avg_adaptation_error = np.mean(adaptation_errors[-20:]) if adaptation_errors else 0
            print(f" Average Reward (last 20): {avg_reward:.2f}")
            print(f" Average Adaptation Error (last 20): {avg_adaptation_error:.3f}")

    return episode_rewards, agent, environment, adaptation_errors

# ========================
# SECTION 10: ADAPTATION ERROR MODULE - COMPLETE IMPLEMENTATION
# ========================

class AdaptationErrorTracker:
    """
    Tracks adaptation error when preferences change
    Measures how well the agent adapts to new preference vectors
    """
    def __init__(self, window_size=10):
        self.window_size = window_size
        self.preference_history = []
        self.reward_history = []
        self.adaptation_errors = []

    def add_experience(self, preference_vector, reward):
        """Add new preference-reward pair"""
        self.preference_history.append(preference_vector.copy())
        self.reward_history.append(reward)

        # Keep only recent history
        if len(self.preference_history) > self.window_size * 2:
            self.preference_history = self.preference_history[-self.window_size * 2:]
            self.reward_history = self.reward_history[-self.window_size * 2:]

    def compute_adaptation_error(self, current_preference, expected_reward, actual_reward):
        """
        Compute adaptation error when preference changes
        """
        if len(self.preference_history) < self.window_size:
            return 0.0

        # Find similar past preferences
        similar_rewards = []
        for i, past_pref in enumerate(self.preference_history[-self.window_size:]):
            similarity = 1.0 - np.linalg.norm(current_preference - past_pref)
            if similarity > 0.8:  # Similar preference threshold
                similar_rewards.append(self.reward_history[-(self.window_size-i)])

        if len(similar_rewards) > 0:
            expected_performance = np.mean(similar_rewards)
            adaptation_error = abs(actual_reward - expected_performance)
            self.adaptation_errors.append(adaptation_error)
            return adaptation_error

        return 0.0

    def get_adaptation_penalty(self):
        """Get penalty based on recent adaptation errors"""
        if len(self.adaptation_errors) == 0:
            return 0.0

        recent_errors = self.adaptation_errors[-5:]  # Last 5 errors
        avg_error = np.mean(recent_errors)

        # Penalty increases with adaptation error
        penalty = min(avg_error * 0.1, 100.0)  # Cap at 100
        return penalty

def calculate_adaptation_error(agent, environment, current_pref, previous_pref=None, adaptation_tracker=None):
    """
    Calculate adaptation error when switching between preferences
    Based on research implementation using Q-learning approach
    """
    if previous_pref is None or adaptation_tracker is None:
        return 0.0

    # Test current performance with current preference
    test_state = environment.get_current_state()

    with torch.no_grad():
        q_values_current = agent.QNet(test_state, current_pref)
        q_values_previous = agent.QNet(test_state, previous_pref)

    # Calculate preference distance
    pref_distance = np.linalg.norm(current_pref - previous_pref)

    # Calculate Q-value difference
    q_diff = torch.mean(torch.abs(q_values_current - q_values_previous)).item()

    # Adaptation error: high when preferences change a lot but Q-values don't adapt proportionally
    adaptation_error = pref_distance * 10.0 - q_diff
    adaptation_error = max(0.0, adaptation_error)  # Only positive errors

    return adaptation_error

# ========================
# Q-LEARNING ADAPTATION ERROR BENCHMARK (From Research)
# ========================

def calculate_q_learning_adaptation_error():
    """
    Research-based Q-learning adaptation error calculation
    This implements the exact method from your research code
    """

    print(" Running Q-Learning Adaptation Error Benchmark...")

    # Discretization (from research)
    energy_bins = np.linspace(0.5, 1.0, 5)
    hash_bins = np.linspace(7, 12, 5)
    token_bins = np.linspace(1.5, 3.5, 5)

    # Actions (from research)
    actions = [0.0, 0.25, 0.5, 0.75, 1.0]  # mining ratios
    Q = np.zeros((6, 6, 6, len(actions)))  # Q-table

    # Reward function (from research)
    def get_reward(mining_ratio, energy_price, hash_price, token_price, preference):
        P_total = 1_000_000
        P_mining = mining_ratio * P_total
        P_infer = (1 - mining_ratio) * P_total
        hash_per_watt = 1.0
        token_per_watt = 3.33

        mining_revenue = P_mining * hash_price * hash_per_watt
        infer_revenue = P_infer * token_price * token_per_watt
        total_revenue = mining_revenue + infer_revenue

        cost = P_total * energy_price
        profit = total_revenue - cost
        energy_eff = total_revenue / P_total
        compute_score = infer_revenue

        return (
            preference[0] * profit +
            preference[1] * energy_eff +
            preference[2] * compute_score
        )

    # Training parameters (from research)
    alpha, gamma, epsilon = 0.4, 0.3, 0.8
    episodes = 1000  # Reduced for faster execution
    pref_train = [0.6, 0.2, 0.2]  # Train preference: Profit-weighted
    train_rewards = []

    # Training phase
    for ep in range(episodes):
        e = np.random.uniform(0.5, 1.0)
        h = np.random.uniform(7.0, 12.0)
        t = np.random.uniform(1.5, 3.5)

        s = (np.digitize(e, energy_bins), np.digitize(h, hash_bins), np.digitize(t, token_bins))
        a = np.random.randint(len(actions)) if random.random() < epsilon else np.argmax(Q[s])

        mining_ratio = actions[a]
        r = get_reward(mining_ratio, e, h, t, pref_train)
        train_rewards.append(r)

        # Next state
        e2, h2, t2 = np.random.uniform(0.5, 1.0), np.random.uniform(7.0, 12.0), np.random.uniform(1.5, 3.5)
        s2 = (np.digitize(e2, energy_bins), np.digitize(h2, hash_bins), np.digitize(t2, token_bins))
        a2 = np.argmax(Q[s2])

        Q[s][a] += alpha * (r + gamma * Q[s2][a2] - Q[s][a])

    # Testing phase with new preference
    pref_test = [0.3, 0.1, 0.6]  # New preference (compute-focused)
    test_rewards = []

    for _ in range(200):  # Reduced for faster execution
        e = np.random.uniform(0.5, 1.0)
        h = np.random.uniform(7.0, 12.0)
        t = np.random.uniform(1.5, 3.5)

        s = (np.digitize(e, energy_bins), np.digitize(h, hash_bins), np.digitize(t, token_bins))
        a = np.argmax(Q[s])

        mining_ratio = actions[a]
        r = get_reward(mining_ratio, e, h, t, pref_test)
        test_rewards.append(r)

    # Adaptation error calculation (from research)
    R_optimal = sum(train_rewards[-200:])  # Last 200 training rewards
    R_test = sum(test_rewards)
    adaptation_error = abs(R_test - R_optimal) / abs(R_optimal) if R_optimal != 0 else 0

    print(f" Q-Learning Benchmark Results:")
    print(f"   Optimal reward (train): {R_optimal:.2f}")
    print(f"   Actual reward (test): {R_test:.2f}")
    print(f"   Adaptation Error: {adaptation_error:.4f}")

    return adaptation_error, train_rewards, test_rewards

# ========================
# SECTION 11: ENHANCED LEARNING LOOP WITH ADAPTATION ERROR
# ========================

def test_reward_function():
    """ Test and demonstrate the reward function with research-based environment"""

    print(" TESTING REWARD FUNCTION (Research Version)")
    print("=" * 60)

    env = Environment()

    # Test different preference vectors
    test_preferences = [
        np.array([0.8, 0.1, 0.1]),    # Profit-focused
        np.array([0.3, 0.6, 0.1]),    # Energy-saving focused
        np.array([0.3, 0.1, 0.6]),    # Compute-utilization focused
        np.array([0.33, 0.33, 0.34])  # Balanced
    ]

    test_actions = [0, 3, 5, 7, 10]  # Different allocation ratio strategies

    for i, pref in enumerate(test_preferences):
        print(f"\n Preference {i+1}: Profit={pref[0]:.1f}, Energy={pref[1]:.1f}, Compute={pref[2]:.1f}")

        for action in test_actions:
            if action < len(ACTION_SPACE_RATIOS):
                infer_ratio, mining_ratio = ACTION_SPACE_RATIOS[action]
                new_state, reward = env.step(action, pref)

                print(f"   Action {action}: Inference={infer_ratio:.1f}, Mining={mining_ratio:.1f} -> Reward: {reward:.2f}")
            else:
                break

    return env

# Add this to main function call
# ========================
# SECTION 12: MAIN EXECUTION WITH ADAPTATION ERROR
# ========================

def main():
    """Main execution function with adaptation error tracking"""

    print(" MOMI MARA - Multi-Objective Trading System")
    print(" Running in Simulation Mode (No API Required)")
    print(" WITH ADAPTATION ERROR TRACKING")
    print("=" * 60)

    # Run Q-learning benchmark first
    print("\n Running Q-Learning Adaptation Error Benchmark...")
    q_adaptation_error, q_train_rewards, q_test_rewards = calculate_q_learning_adaptation_error()

    # Test reward function
    print("\n Testing reward function...")
    test_reward_function()

    # Run training with adaptation error
    print("\n🏋️ Starting Multi-Objective RL training with adaptation error...")
    rewards, trained_agent, env, adaptation_errors = Learning_Loop_with_Adaptation(
        ep_num=100,
        site_num=5,
        sample_coef=1.0
    )

    # Plot results
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

    # Plot RL training rewards
    ax1.plot(rewards)
    ax1.set_title('Multi-Objective RL Training Rewards')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Reward')
    ax1.grid(True)

    # Plot RL adaptation errors
    if adaptation_errors:
        ax2.plot(adaptation_errors, color='red')
        ax2.set_title('RL Adaptation Errors Over Time')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Adaptation Error')
        ax2.grid(True)

    # Plot Q-learning benchmark
    ax3.plot(q_train_rewards, label="Q-Learning Training", alpha=0.7)
    ax3.axhline(y=np.mean(q_test_rewards), color='red', linestyle='--',
                label=f"Q-Learning Test (Adaptation Error: {q_adaptation_error:.3f})")
    ax3.set_title('Q-Learning Benchmark')
    ax3.set_xlabel('Episode')
    ax3.set_ylabel('Reward')
    ax3.legend()
    ax3.grid(True)

    # Comparison plot
    if adaptation_errors:
        rl_adaptation_error = np.mean(adaptation_errors[-20:]) if len(adaptation_errors) >= 20 else np.mean(adaptation_errors)
        methods = ['Q-Learning\nBenchmark', 'Multi-Objective\nRL (Ours)']
        errors = [q_adaptation_error, rl_adaptation_error]

        ax4.bar(methods, errors, color=['orange', 'blue'], alpha=0.7)
        ax4.set_title('Adaptation Error Comparison')
        ax4.set_ylabel('Adaptation Error')
        ax4.grid(True, alpha=0.3)

        # Add value labels on bars
        for i, v in enumerate(errors):
            ax4.text(i, v + max(errors) * 0.01, f'{v:.3f}', ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

    print(" Training completed!")

    # Analyze adaptation performance
    if adaptation_errors:
        avg_adaptation_error = np.mean(adaptation_errors)
        final_adaptation_errors = np.mean(adaptation_errors[-20:]) if len(adaptation_errors) >= 20 else np.mean(adaptation_errors)
        improvement = ((avg_adaptation_error - final_adaptation_errors) / avg_adaptation_error * 100) if avg_adaptation_error != 0 else 0

        print(f"\n ADAPTATION ANALYSIS:")
        print(f"   Q-Learning Benchmark Adaptation Error: {q_adaptation_error:.3f}")
        print(f"   Our Multi-Objective RL Average: {avg_adaptation_error:.3f}")
        print(f"   Our Multi-Objective RL Final: {final_adaptation_errors:.3f}")
        print(f"   Our Method Improvement: {improvement:.1f}%")

        if final_adaptation_errors < q_adaptation_error:
            print(f"   Our method performs {((q_adaptation_error - final_adaptation_errors) / q_adaptation_error * 100):.1f}% better!")
        else:
            print(f"    Q-Learning benchmark performs {((final_adaptation_errors - q_adaptation_error) / q_adaptation_error * 100):.1f}% better")

    # Test the trained agent
    print("\n Testing trained agent with different preferences...")
    test_preferences = [
        np.array([0.8, 0.1, 0.1]),  # Profit-focused
        np.array([0.3, 0.6, 0.1]),  # Energy-saving focused
        np.array([0.3, 0.1, 0.6]),  # Compute-utilization focused
    ]

    for i, test_pref in enumerate(test_preferences):
        test_state = env.get_current_state()

        with torch.no_grad():
            q_values = trained_agent.QNet(test_state, test_pref)
            best_action = q_values.argmax().item()

        pref_type = ["Profit-focused", "Energy-focused", "Compute-focused"][i]
        infer_ratio, mining_ratio = ACTION_SPACE_RATIOS[best_action]

        print(f" {pref_type}: Preference {test_pref} -> Action {best_action}")
        print(f"   Allocation Ratios: {infer_ratio:.1f} inference, {mining_ratio:.1f} mining")
        print(f"   Power Split: {infer_ratio*100:.0f}% inference, {mining_ratio*100:.0f}% mining")

    return trained_agent, env, rewards, adaptation_errors

if __name__ == "__main__":
    print(" STARTING MOMI MARA MULTI-OBJECTIVE TRADING SYSTEM")
    print(" With Research-Based Adaptation Error Analysis")
    print("=" * 70)

    try:
        trained_agent, environment, reward_history, adaptation_error_history = main()

        print("\n SYSTEM EXECUTION COMPLETED SUCCESSFULLY!")
        print(f" Total episodes trained: {len(reward_history)}")
        print(f" Final average reward: {np.mean(reward_history[-10:]):.2f}")
        if adaptation_error_history:
            print(f" Final adaptation error: {np.mean(adaptation_error_history[-10:]):.3f}")

    except Exception as e:
        print(f"\n Error during execution: {e}")
        print(" Please check all dependencies are installed:")
        print("   pip install torch numpy matplotlib pandas requests")
        import traceback
        traceback.print_exc()